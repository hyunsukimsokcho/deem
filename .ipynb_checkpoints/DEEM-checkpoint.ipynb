{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import copy \n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cluster import KMeans\n",
    "from Component.duplicator import Duplicator\n",
    "from Component.reweighing import Reweighing\n",
    "from Component.preprocessing import Preprocessing\n",
    "from Component.utils import timer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and Duplicating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADDED 557 MANY RAW DUPLICATES\n"
     ]
    }
   ],
   "source": [
    "#dataset = \"AdultCensus\"\n",
    "dataset = \"CompasRecidivism\"\n",
    "#dataset = \"GermanBank\"\n",
    "\n",
    "feature = \"@sex\"\n",
    "duplicator = Duplicator(category=dataset)\n",
    "\n",
    "if dataset == \"AdultCensus\" :\n",
    "    p_group = \"Male\"\n",
    "    up_group = \"Female\"\n",
    "elif dataset == \"GermanBank\":\n",
    "    p_group = \"Male\"\n",
    "    up_group = \"Female\"\n",
    "elif dataset == \"CompasRecidivism\":\n",
    "    p_group = \"Female\"\n",
    "    up_group = \"Male\"\n",
    "\n",
    "raw_train_data, raw_test_data = duplicator.train_data, duplicator.test_data\n",
    "duplicator.add_raw_duplicate(4)\n",
    "duplicate_train_data, test_data = duplicator.train_data, duplicator.test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = Preprocessing(dataset=dataset)\n",
    "raw_train_x, raw_train_y, raw_test_x, raw_test_y = data_preprocessing.transform(train_data=raw_train_data, test_data=raw_test_data)\n",
    "duplicate_train_x, duplicate_train_y, duplicate_test_x, duplicate_test_y = data_preprocessing.transform(train_data=duplicate_train_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_priors</th>\n",
       "      <th>two_yr_recidivism_0</th>\n",
       "      <th>two_yr_recidivism_1</th>\n",
       "      <th>score_factor_0</th>\n",
       "      <th>score_factor_1</th>\n",
       "      <th>@age_Above_FourtyFive_0</th>\n",
       "      <th>@age_Above_FourtyFive_1</th>\n",
       "      <th>@age_Below_TwentyFive_0</th>\n",
       "      <th>@age_Below_TwentyFive_1</th>\n",
       "      <th>@african_american_0</th>\n",
       "      <th>@african_american_1</th>\n",
       "      <th>@asian_0</th>\n",
       "      <th>@asian_1</th>\n",
       "      <th>@hispanic_0</th>\n",
       "      <th>@hispanic_1</th>\n",
       "      <th>@native_american_0</th>\n",
       "      <th>@native_american_1</th>\n",
       "      <th>@other_0</th>\n",
       "      <th>@other_1</th>\n",
       "      <th>@sex_Female</th>\n",
       "      <th>@sex_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.683442</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.170347</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.043100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_priors  two_yr_recidivism_0  two_yr_recidivism_1  score_factor_0  \\\n",
       "0         -0.683442                    1                    0               1   \n",
       "1         -0.683442                    0                    1               1   \n",
       "2          0.170347                    0                    1               1   \n",
       "3         -0.683442                    1                    0               1   \n",
       "4         -0.043100                    1                    0               1   \n",
       "\n",
       "   score_factor_1  @age_Above_FourtyFive_0  @age_Above_FourtyFive_1  \\\n",
       "0               0                        0                        1   \n",
       "1               0                        1                        0   \n",
       "2               0                        1                        0   \n",
       "3               0                        1                        0   \n",
       "4               0                        1                        0   \n",
       "\n",
       "   @age_Below_TwentyFive_0  @age_Below_TwentyFive_1  @african_american_0  \\\n",
       "0                        1                        0                    1   \n",
       "1                        1                        0                    0   \n",
       "2                        0                        1                    0   \n",
       "3                        1                        0                    1   \n",
       "4                        1                        0                    1   \n",
       "\n",
       "   @african_american_1  @asian_0  @asian_1  @hispanic_0  @hispanic_1  \\\n",
       "0                    0         1         0            1            0   \n",
       "1                    1         1         0            1            0   \n",
       "2                    1         1         0            1            0   \n",
       "3                    0         1         0            1            0   \n",
       "4                    0         1         0            1            0   \n",
       "\n",
       "   @native_american_0  @native_american_1  @other_0  @other_1  @sex_Female  \\\n",
       "0                   1                   0         0         1            0   \n",
       "1                   1                   0         1         0            0   \n",
       "2                   1                   0         1         0            0   \n",
       "3                   1                   0         0         1            0   \n",
       "4                   1                   0         0         1            0   \n",
       "\n",
       "   @sex_Male  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc : 0.656\n",
      "Test acc : 0.675\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'duplicate_train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3b852d86b15d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduplicate_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduplicate_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training acc : %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduplicate_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduplicate_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test acc : %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduplicate_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduplicate_test_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'duplicate_train_x' is not defined"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l1')\n",
    "model.fit(raw_train_x, raw_train_y)\n",
    "print(\"Training acc : %0.3f\" % model.score(raw_train_x, raw_train_y))\n",
    "print(\"Test acc : %0.3f\" % model.score(raw_test_x, raw_test_y))\n",
    "\n",
    "model = LogisticRegression(penalty='l1')\n",
    "model.fit(duplicate_train_x, duplicate_train_y)\n",
    "print(\"Training acc : %0.3f\" % model.score(duplicate_train_x, duplicate_train_y))\n",
    "print(\"Test acc : %0.3f\" % model.score(duplicate_test_x, duplicate_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Component.poisoner import Poisoner\n",
    "\n",
    "poisoner = Poisoner(raw_train_x, raw_train_y, raw_test_x, raw_test_y, 0.2)\n",
    "poisoner.add_poison(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoner.poison_x[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure accuracy and demographic parity on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l1')\n",
    "model.fit(duplicate_train_x, duplicate_train_y)\n",
    "print(\"Training acc : %0.3f\" % model.score(duplicate_train_x, duplicate_train_y))\n",
    "print(\"Test acc : %0.3f\" % model.score(duplicate_test_x, duplicate_test_y))\n",
    "\n",
    "reweighing = Reweighing(duplicate_train_x, duplicate_train_y, feature, p_group, up_group)\n",
    "original_dp = reweighing.fairness_measure(model)\n",
    "print(\"Demographic parity : %0.3f\" % original_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure accuracy and demographic parity on poisoned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(x_train, y_train, x_test, y_test, fair=False, weight=None):\n",
    "    mitigated_model = LogisticRegression(penalty='l1')\n",
    "    reweighing.change_data(x_train, y_train)\n",
    "    \n",
    "    if fair:\n",
    "        sample_weight = reweighing.calculate_weight()\n",
    "    elif weight is not None:\n",
    "        sample_weight = weight\n",
    "    else:\n",
    "        sample_weight = np.ones(len(x_train))\n",
    "    mitigated_model.fit(x_train, y_train, sample_weight=sample_weight)\n",
    "    print(\"Training acc : %0.3f\" % model.score(x_train, y_train))\n",
    "    print(\"Test acc : %0.3f\" % model.score(x_test, y_test))\n",
    "\n",
    "    demo_parity = reweighing.fairnesss_measure(model)\n",
    "    print(\"Demographic parity : %0.3f\" % demo_parity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1 : Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data, cleaned_label = cleaning(poisoned_data, poisoned_label)\n",
    "show_result(cleaned_data, cleaned_label, duplicate_test_x, duplicate_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2 : Data Sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_data, sanitized_label = sanitization(poisoned_data, poisoned_label)\n",
    "show_result(sanitized_data, sanitized_label, duplicate_test_x, duplicate_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3 : Unfiarness Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(poisoned_data, poisoned_label, duplicate_test_x, duplicate_test_y, True):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 4 : Data Cleaning -> Data Sanitization -> Unfairness Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data, cleaned_label = cleaning(poisoned_data, poisoned_label)\n",
    "sanitized_data, sanitized_label = sanitization(cleaned_data, cleaned_label)\n",
    "show_result(sanitized_data, sanitized_label, duplicate_test_x, duplicate_test_y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 5 : Unfairness Mitigation -> Data Sanitization -> Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweighing.change_data(poisoned_data, poisoned_label)\n",
    "sample_weight = reweighing.calculate_weight()\n",
    "\n",
    "cleaned_data, cleaned_label = cleaning(poisoned_data, poisoned_label)\n",
    "sanitized_data, sanitized_label = sanitization(cleaned_data, cleaned_label)\n",
    "\n",
    "show_result(sanitized_data, sanitized_label, x_test, y_test, weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 6 : MLClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data, train):\n",
    "    data_copy = data.copy()\n",
    "    data_copy[\"Target\"] = data_copy[\"Target\"].apply(lambda x:0 if (x=='<=50K' or x=='<=50K.') else 1)\n",
    "    x_data = data_copy.drop('Target', axis =1)\n",
    "    y_data = data_copy['Target']\n",
    "\n",
    "    num_data = x_data.select_dtypes(include=\"int\")\n",
    "    cat_data = x_data.select_dtypes(include='object')\n",
    "\n",
    "    if train==1:\n",
    "        num_data = pd.DataFrame(scalar.fit_transform(num_data), columns=num_data.columns)\n",
    "    else:\n",
    "        num_data = pd.DataFrame(scalar.transform(num_data), columns=num_data.columns)\n",
    "    cat_data = pd.get_dummies(cat_data)\n",
    "\n",
    "    x_data = pd.concat([num_data, cat_data], axis=1)\n",
    "    return x_data, y_data\n",
    "\n",
    "names = [\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"]\n",
    "\n",
    "train_data = pd.read_csv('Dataset/AdultCensus/train.data', names=names, \n",
    "             sep=' *, *', na_values='?')\n",
    "test_data  = pd.read_csv('Dataset/AdultCensus/test.data', names=names, \n",
    "             sep=' *, *', skiprows=1, na_values='?')\n",
    "\n",
    "train_data.drop(['fnlwgt', 'Education'], axis=1, inplace=True)\n",
    "test_data.drop(['fnlwgt', 'Education'], axis=1, inplace=True)\n",
    "train_data = train_data.dropna().reset_index(drop=True)\n",
    "test_data = test_data.dropna().reset_index(drop=True)\n",
    "\n",
    "scalar = StandardScaler()\n",
    "x_train, y_train = data_preprocessing(train_data, 1)\n",
    "x_test, y_test = data_preprocessing(test_data, 0)\n",
    "\n",
    "missing_cols = set(x_train.columns) - set(x_test.columns)\n",
    "for column in missing_cols:\n",
    "    print(column)\n",
    "    x_test[column] = 0\n",
    "x_test = x_test[x_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0, penalty='l1')\n",
    "#model = LinearSVC(penalty='l1', dual=False, loss='l2')\n",
    "model.fit(x_train, y_train)\n",
    "print(model.score(x_train, y_train))\n",
    "print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index1 = y_train.index[y_train.values==1].tolist()\n",
    "print(len(test_index1))\n",
    "test_index2 = y_train.index[y_train.values==0].tolist()\n",
    "print(len(test_index2))\n",
    "\n",
    "test_index3 = y_test.index[y_test.values==1].tolist()\n",
    "print(len(test_index3))\n",
    "test_index4 = y_test.index[y_test.values==0].tolist()\n",
    "print(len(test_index4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_poisoing(data):\n",
    "    return data\n",
    "\n",
    "def data_duplicate(data):\n",
    "    return data\n",
    "\n",
    "def cleaning(data):\n",
    "    return data\n",
    "\n",
    "def sanitization(data):\n",
    "    return data\n",
    "\n",
    "def reweighing(data, label, feature, p_group, up_group):\n",
    "    size = len(data)\n",
    "    p_feature = feature+\"_\"+p_group\n",
    "    up_feature = feature+\"_\"+up_group\n",
    "\n",
    "    up_index = data.index[data[up_feature]==1].tolist()\n",
    "    p_index = data.index[data[p_feature]==1].tolist()\n",
    "    f_index = label.index[label.values==1].tolist()\n",
    "    uf_index = label.index[label.values==0].tolist()\n",
    "\n",
    "    f_up_index = list(set(f_index)&set(up_index))\n",
    "    f_p_index = list(set(f_index)&set(p_index))\n",
    "    uf_up_index = list(set(uf_index)&set(up_index))\n",
    "    uf_p_index = list(set(uf_index)&set(p_index))\n",
    "\n",
    "    weight_f_up = len(f_index) * len(up_index) / (size * len(f_up_index))\n",
    "    weight_f_p = len(f_index) * len(p_index) / (size * len(f_p_index))\n",
    "    weight_uf_up = len(uf_index) * len(up_index) / (size * len(uf_up_index))\n",
    "    weight_uf_p = len(uf_index) * len(p_index) / (size * len(uf_p_index))\n",
    "\n",
    "    weight = np.zeros(size)\n",
    "    weight[f_up_index] = weight_f_up\n",
    "    weight[f_p_index] = weight_f_p\n",
    "    weight[uf_up_index] = weight_uf_up\n",
    "    weight[uf_p_index] = weight_uf_p\n",
    "    return weight\n",
    "\n",
    "def fairness_measure(data, label, model, feature, p_group, up_group):\n",
    "    p_feature = feature+\"_\"+p_group\n",
    "    up_feature = feature+\"_\"+up_group\n",
    "    prediction = model.predict(data)\n",
    "    \n",
    "    p_index = data.index[data[p_feature]==1].tolist()\n",
    "    up_index = data.index[data[up_feature]==1].tolist()\n",
    "    \n",
    "    p_pred = prediction[p_index]\n",
    "    up_pred = prediction[up_index]\n",
    "    p_ratio = np.sum(p_pred)/len(p_pred)\n",
    "    up_ratio = np.sum(up_pred)/len(up_pred)\n",
    "    print(p_ratio, up_ratio, p_ratio/up_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_measure(x_train, y_train, model, \"Sex\", \"Female\", \"Male\" )\n",
    "sample_weight = reweighing(x_train, y_train, \"Sex\", \"Female\", \"Male\")\n",
    "\n",
    "mitigated_model = LogisticRegression(random_state=0, penalty='l1')\n",
    "mitigated_model.fit(x_train, y_train, sample_weight=sample_weight)\n",
    "print(mitigated_model.score(x_train, y_train))\n",
    "print(mitigated_model.score(x_test, y_test))\n",
    "\n",
    "fairness_measure(x_train, y_train, mitigated_model, \"Sex\", \"Female\", \"Male\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
